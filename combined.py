# -*- coding: utf-8 -*-
"""combined_minor5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-AP8uFhSx3kydCYRtmDRwjMyD-FOmVql
"""

# Import necessary libraries
import os
import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import librosa
import librosa.display
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from keras.models import Model
from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten, concatenate
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# Function to preprocess text data
def preprocess_text(file_path):
    data = pd.read_csv(file_path, sep=';')
    happy_emotions = ['joy', 'love', 'surprise']
    data['label'] = data['emotion'].apply(lambda x: 1 if x in happy_emotions else 0)
    return data

# Load and preprocess text datasets
dataset_dir = '/content/'  # Adjust if necessary
train_text_path = os.path.join(dataset_dir, 'train.txt')
val_text_path = os.path.join(dataset_dir, 'val.txt')

train_text_data = preprocess_text(train_text_path)
val_text_data = preprocess_text(val_text_path)

# Prepare audio file paths and labels
audio_dataset_dir = '/content/drive/MyDrive/emotion_dataset/Emotions/'  # Adjust path
audio_paths = []
audio_labels = []

for dirname, _, filenames in os.walk(audio_dataset_dir):
    for filename in filenames:
        if filename.endswith('.wav'):
            file_path = os.path.join(dirname, filename)
            audio_paths.append(file_path)
            label = os.path.basename(dirname).lower()
            audio_labels.append(label)

# Create a DataFrame for audio data
audio_df = pd.DataFrame({'speech': audio_paths, 'emotion': audio_labels})

# Map emotions to binary labels consistent with text data
happy_emotions = ['joy', 'love', 'surprise']
audio_df['label'] = audio_df['emotion'].apply(lambda x: 1 if x in happy_emotions else 0)

# For demonstration, we'll create synthetic IDs to merge datasets
train_text_data['id'] = range(len(train_text_data))
audio_df['id'] = range(len(audio_df))

# Merge text and audio data on 'id' (ensure alignment in your actual data)
multimodal_data = pd.merge(train_text_data, audio_df, on='id', suffixes=('_text', '_audio'))

# Drop unnecessary columns
multimodal_data = multimodal_data[['text', 'speech', 'label_text']]
multimodal_data.rename(columns={'label_text': 'label'}, inplace=True)

def extract_mfcc(filename, n_mfcc=40):
    try:
        y, sr = librosa.load(filename, duration=3, offset=0.5)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
        mfcc_scaled = np.mean(mfcc.T, axis=0)
        return mfcc_scaled
    except Exception as e:
        print(f"Error processing {filename}: {e}")
        return None

# Apply MFCC extraction
multimodal_data['mfcc'] = multimodal_data['speech'].apply(lambda x: extract_mfcc(x))

# Drop rows where MFCC extraction failed
multimodal_data = multimodal_data.dropna(subset=['mfcc'])

# Extract MFCC features and add to audio_df
audio_df['mfcc'] = audio_df['speech'].apply(lambda x: extract_mfcc(x))

# Drop any rows where MFCC may have failed
audio_df.dropna(subset=['mfcc'], inplace=True)

# Assign IDs
train_text_data['id'] = train_text_data.index
audio_df['id'] = audio_df.index

# Merge datasets on 'id' (ensuring proper alignment)
# Note: Ensure that IDs correspond to the same samples
multimodal_data = pd.merge(train_text_data, audio_df, on='id', suffixes=('_text', '_audio'))

# Check the resulting DataFrame columns
print("Columns in multimodal_data after merge:", multimodal_data.columns)

# Select relevant columns, including 'mfcc'
# Adjust column names if 'mfcc' has a suffix
multimodal_data = multimodal_data[['text', 'mfcc', 'label_text']]
multimodal_data.rename(columns={'label_text': 'label'}, inplace=True)

# Proceed with the rest of your code
# Ensure 'mfcc' feature vectors are consistent in length
mfcc_lengths = multimodal_data['mfcc'].apply(lambda x: len(x))
if len(mfcc_lengths.unique()) > 1:
    print("Inconsistent MFCC feature lengths detected.")
    # Handle inconsistencies here (e.g., padding/truncating)

# Convert 'mfcc' column to a NumPy array
audio_features = np.array(multimodal_data['mfcc'].tolist())
print('audio_features.shape:', audio_features.shape)

# Reshape 'audio_features' if necessary
if len(audio_features.shape) == 2:
    audio_features = audio_features.reshape(audio_features.shape[0], audio_features.shape[1], 1)
else:
    print("Unexpected shape for 'audio_features':", audio_features.shape)
    # Handle the error or adjust the reshaping accordingly

# Proceed with labels
labels = multimodal_data['label'].values

# Split data into training and validation sets
X_text_train, X_text_val, X_audio_train, X_audio_val, y_train, y_val = train_test_split(
    multimodal_data['text'], audio_features, labels, test_size=0.2, random_state=42, stratify=labels)

# Import necessary libraries
import os
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display
from sklearn.utils import class_weight

# Function to preprocess the data
def preprocess(file_path):
    data = pd.read_csv(file_path, sep=';')
    # Map emotions to binary labels: 1 for 'joy', 'love', 'surprise', else 0
    happy_emotions = ['joy', 'love', 'surprise']
    data['hos'] = data['emotion'].apply(lambda x: 1 if x in happy_emotions else 0)
    return data

# Define the dataset directory
dataset_dir = '/content/'  # Adjust if you're using Google Drive

# Construct paths to the training and validation dataset files
train_path = os.path.join(dataset_dir, 'train.txt')
val_path = os.path.join(dataset_dir, 'val.txt')

# Load and preprocess the datasets
train_data = preprocess(train_path)
val_data = preprocess(val_path)

# Custom Layer to wrap the TensorFlow Hub Layer
class CustomHubLayer(tf.keras.layers.Layer):
    def __init__(self, model_url, **kwargs):
        super(CustomHubLayer, self).__init__(**kwargs)
        self.hub_layer = hub.KerasLayer(model_url, output_shape=[20], input_shape=[], dtype=tf.string, trainable=True)

    def call(self, inputs):
        return self.hub_layer(inputs)

# Specify the URL to the pre-trained embedding model (BERT-based or Universal Sentence Encoder)
model_url = "https://tfhub.dev/google/universal-sentence-encoder/4"

# Build the model using the functional API
inputs = tf.keras.Input(shape=[], dtype=tf.string)
x = CustomHubLayer(model_url)(inputs)

# Add layers with regularization and batch normalization
x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.5)(x)

x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.5)(x)

x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.5)(x)

outputs = tf.keras.layers.Dense(1)(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Display model architecture
model.summary()

# Audio input
audio_input = Input(shape=(40, 1), name='audio_input')

# CNN layers for audio
x_audio = Conv1D(256, 5, padding='same', activation='relu')(audio_input)
x_audio = BatchNormalization()(x_audio)
x_audio = MaxPooling1D(pool_size=2)(x_audio)
x_audio = Dropout(0.3)(x_audio)

x_audio = Conv1D(128, 5, padding='same', activation='relu')(x_audio)
x_audio = BatchNormalization()(x_audio)
x_audio = MaxPooling1D(pool_size=2)(x_audio)
x_audio = Dropout(0.3)(x_audio)

x_audio = Conv1D(64, 5, padding='same', activation='relu')(x_audio)
x_audio = BatchNormalization()(x_audio)
x_audio = MaxPooling1D(pool_size=2)(x_audio)
x_audio = Dropout(0.3)(x_audio)

x_audio = Flatten()(x_audio)
x_audio = Dense(64, activation='relu')(x_audio)
x_audio = Dropout(0.3)(x_audio)

# Combine the outputs from both modalities
combined = concatenate([x, x_audio])

# Final dense layers
z = Dense(64, activation='relu')(combined)
z = Dropout(0.5)(z)
z = Dense(32, activation='relu')(z)
z = Dropout(0.5)(z)

# Output layer
output = Dense(1, activation='sigmoid')(z)

# Define the model with inputs and output
model = Model(inputs=[inputs, audio_input], outputs=output)

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Model summary
model.summary()

# Compute class weights
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))
print("Class weights:", class_weights)

print("Shape of text data:", X_text_train_str.shape)
print("Shape of audio data:", X_audio_train.shape)

from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, concatenate
from tensorflow.keras.models import Model

# Text input branch
text_input = Input(shape=(max_seq_length,), name='input_layer')  # Match the shape of the padded sequences
embedding_layer = Embedding(input_dim=max_words, output_dim=128, input_length=max_seq_length)(text_input)
lstm_layer = LSTM(128)(embedding_layer)

# Audio input branch
audio_input = Input(shape=(40, 1), name='audio_input')  # Match the audio input shape
conv_layer = Conv1D(64, kernel_size=3, activation='relu')(audio_input)
pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)
flatten_layer = Flatten()(pooling_layer)

# Concatenate the outputs of both branches
merged = concatenate([lstm_layer, flatten_layer])

# Dense layers and output
dense_layer = Dense(64, activation='relu')(merged)
output_layer = Dense(1, activation='sigmoid')(dense_layer)

# Define the model
model = Model(inputs=[text_input, audio_input], outputs=output_layer)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()

# Train the model
history = model.fit(
    x={'input_layer': X_text_train_seq_padded, 'audio_input': X_audio_train},
    y=y_train,
    epochs=50,
    batch_size=64,
    validation_data=({'input_layer': X_text_val_seq_padded, 'audio_input': X_audio_val}, y_val),
    class_weight=class_weights,
    callbacks=[early_stopping],
    verbose=1
)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize the text
tokenizer = Tokenizer(num_words=max_tokens)
tokenizer.fit_on_texts(X_text_train)

# Convert text to sequences
X_text_train_seq = tokenizer.texts_to_sequences(X_text_train)
X_text_val_seq = tokenizer.texts_to_sequences(X_text_val)

# Pad sequences to the same length
X_text_train_padded = pad_sequences(X_text_train_seq, maxlen=output_sequence_length)
X_text_val_padded = pad_sequences(X_text_val_seq, maxlen=output_sequence_length)

# Update your model's input layer for text
text_input = Input(shape=(output_sequence_length,), name='input_layer')  # Input is an integer sequence
# Proceed with embedding or other layers

# Evaluate on validation set
val_loss, val_accuracy = model.evaluate(
    {'text_input': X_text_val_tensor, 'audio_input': X_audio_val},
    y_val,
    verbose=1
)
print(f"\nValidation Loss: {val_loss:.4f}")
print(f"Validation Accuracy: {val_accuracy:.4f}")

# Plot accuracy
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Multimodal Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Multimodal Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Function to get user input and predict mental health score
def get_user_input_predictions(model):
    print("\nPlease answer the following questions to assess your mental health:\n")
    questions = [
        '1. How would you describe your experience at your workplace/college/school in the past few days? ',
        '2. How do you like to spend your leisure time? How do you feel after it? ',
        '3. Life has its ups and downs. How do you manage your emotions after failures? ',
        '4. Are there any improvements/decline in your salary/grades? ',
        '5. In a broad sense, how would you describe the way your life is going on? '
    ]

    answers = []
    for q in questions:
        ans = input(q)
        answers.append(ans)

    # Collect audio input from the user
    print("\nPlease provide an audio recording of you expressing your feelings. (This step assumes that you have recorded and saved the audio as 'user_input.wav')")
    user_audio_file = '/content/resonate 1727881759017.wav'  # Path to user's audio input

    # Extract features from audio
    user_mfcc = extract_mfcc(user_audio_file)
    if user_mfcc is None:
        print("Error processing audio input.")
        return

    user_mfcc = np.expand_dims(user_mfcc, axis=0)
    user_mfcc = np.expand_dims(user_mfcc, axis=2)

    # Prepare text input
    user_text = ' '.join(answers)
    user_text_tensor = tf.convert_to_tensor([user_text])

    # Make prediction
    probability = model.predict({'text_input': user_text_tensor, 'audio_input': user_mfcc})
    score = probability[0][0] * 100  # Convert to percentage

    # Provide feedback
    print(f'\nYour mental health score is: {score:.2f}%')
    if score < 25:
        print("You are going through a challenging phase in life. Consider seeking support.")
    elif score < 50:
        print("You're facing some difficulties, but there is room for improvement.")
    elif score < 75:
        print("Your mental health is generally good, but strive for maintenance.")
    else:
        print("Your mental health looks excellent! Keep enjoying life.")
